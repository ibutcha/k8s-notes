{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"eks/cluster/","title":"EKS Cluster","text":""},{"location":"eks/cluster/#prerequisite","title":"Prerequisite","text":"<ul> <li>aws-cli - command line tool to help managing your AWS services. </li> <li>eksctl -  command line tool for creating and managing Kubernetes clusters on Amazon EKS.</li> <li>kubectl - command line tool that you use to communicate with the Kubernetes API server.</li> </ul>"},{"location":"eks/cluster/#step-1-creating-cluster","title":"Step 1: Creating Cluster","text":"<pre><code># Template\n$ eksctl create cluster \\\n--name &lt;cluster-name&gt; \\\n--region &lt;target-aws-region&gt; \\\n--version &lt;k8s-version&gt; \\\n--without-nodegroup\n\n# Example: \n$ eksctl create cluster \\\n--name app-cluster \\\n--region ap-southeast-1 \\\n--version 1.24 \\\n--without-nodegroup \\\n--tags \"Project=Cap Build,Topic=EKS\"\n# Cluster provisioning takes several minutes. While the cluster is being created, several lines of output appear.\n# The last line of output is similar to the following example line.    \n# [\u2713]  EKS cluster \"app-cluster\" in \"ap-southeast-1\" region is ready \n# To verify, get cluster list.\n$ eksctl get cluster\n---\nNAME        REGION      EKSCTL CREATED\napp-cluster ap-southeast-1  True\n</code></pre>"},{"location":"eks/cluster/#step-2-creating-an-iam-oidc-provider-for-your-cluster","title":"Step 2: Creating an IAM OIDC provider for your cluster.","text":"<p>To use some Amazon EKS add-ons, or to enable individual Kubernetes workloads to have specific AWS Identity and Access Management (IAM) permissions <pre><code># Template\n$ eksctl utils associate-iam-oidc-provider --cluster &lt;cluster-name&gt; --approve\n\n# Example:\n$  eksctl utils associate-iam-oidc-provider --cluster app-cluster --approve\n-----\n# [\u2714]  created IAM Open ID Connect provider for cluster \"app-cluster\" in \"ap-southeast-1\"\n</code></pre></p>"},{"location":"eks/cluster/#step-3-create-ec2-keypair","title":"Step 3: Create EC2 Keypair","text":"<p>This will use to EKS Nodes as SSH-Key.</p> <pre><code># Template\n$ aws ec2 create-key-pair \\\n--key-name &lt;key-pair-name&gt; \\\n--key-type rsa \\\n--key-format pem \\\n--query \"KeyMaterial\" \\\n--output text &gt; &lt;key-pair-name&gt;.pem\n\n# Example\n$ aws ec2 create-key-pair \\\n--key-name eks \\\n--key-type rsa \\\n--key-format pem \\\n--query \"KeyMaterial\" \\\n--output text &gt; eks.pem\n\n# Change Permission \n$ chmod 400 eks.pem\n</code></pre>"},{"location":"eks/cluster/#step-4-create-node-group-with-additional-add-ons-in-public-subnets","title":"Step 4: Create Node Group with additional Add-Ons in Public Subnets","text":"<p>Automate the provisioning and lifecycle management of nodes (Amazon EC2 instances) for Amazon EKS Kubernetes clusters.</p> <p>Create Public Node Group <pre><code># Template\n$ eksctl create nodegroup \\\n--cluster=&lt;cluster-name&gt;  \\\n--region=&lt;aws-region&gt; \\\n--name=&lt;nodegroup-name&gt; \\\n--spot \\\n--instance-types=&lt;spot-instance-types&gt;\\\n--nodes=&lt;no-of-nodes&gt; \\\n--nodes-min=&lt;min-no-of-nodes&gt; \\\n--nodes-max=&lt;max-no-of-nodes&gt; \\\n--node-volume-size=&lt;node-volume-size&gt; \\\n--ssh-access \\\n--ssh-public-key=&lt;ssh-key-name&gt; \\\n--managed \\\n--asg-access \\ \n--external-dns-access \\\n--full-ecr-access \\\n--appmesh-access \\\n--alb-ingress-access # Example    \n$  eksctl create nodegroup \\\n--cluster=app-cluster \\\n--region=ap-southeast-1 \\\n--name=app-cluster-ng-public1 \\\n--spot \\\n--instance-types=t3.medium \\\n--nodes=1 \\\n--nodes-min=1 \\\n--nodes-max=4 \\\n--node-volume-size=20 \\\n--ssh-access \\\n--ssh-public-key=eks \\\n--managed \\\n--asg-access \\\n--external-dns-access \\\n--full-ecr-access \\\n--appmesh-access \\\n--alb-ingress-access \\\n--tags \"Project=Cap Build,Topic=EKS\"\n# To Verify, get list NodeGroups in a cluster.\n$ eksctl get nodegroups --cluster=app-cluster\n---\nCLUSTER     NODEGROUP       STATUS  CREATED         MIN SIZE    MAX SIZE    DESIRED CAPACITY    INSTANCE TYPE   IMAGE ID    ASG NAME                            TYPE\napp-cluster app-cluster-ng-public1  ACTIVE  2023-01-06T03:49:32Z    1       4       1           t3.medium   AL2_x86_64  eks-app-cluster-ng-public1-bac2c299-55c5-9493-afb3-76824d7b9de8 managed\n\n# List Nodes in current kubernetes cluster\n$ kubectl get nodes -o wide\n---\nNAME                                                STATUS   ROLES    AGE   VERSION               INTERNAL-IP      EXTERNAL-IP     OS-IMAGE         KERNEL-VERSION                 CONTAINER-RUNTIME\nip-192-168-32-189.ap-southeast-1.compute.internal   Ready    &lt;none&gt;   18m   v1.24.7-eks-fb459a0   192.168.32.189   54.179.130.81   Amazon Linux 2   5.4.226-129.415.amzn2.x86_64   containerd://1.6.6\n</code></pre></p>"},{"location":"eks/cluster/#step-5-create-node-group-in-private-subnets","title":"Step 5: Create Node Group in Private Subnets","text":"<p>Key option for the command is <code>--node-private-networking</code> <pre><code> eksctl create nodegroup \\\n--cluster=app-cluster \\\n--region=ap-southeast-1 \\\n--name=app-cluster-ng-private1 \\\n--spot \\\n--instance-types=t3.medium \\\n--nodes=1 \\\n--nodes-min=1 \\\n--nodes-max=2 \\\n--node-volume-size=20 \\\n--ssh-access \\\n--ssh-public-key=eks \\\n--managed \\\n--asg-access \\\n--external-dns-access \\\n--full-ecr-access \\\n--appmesh-access \\\n--alb-ingress-access \\\n--node-private-networking \\\n--tags \"Project=Cap Build,Topic=EKS\"\n</code></pre></p>"},{"location":"eks/cluster/#step-6-terminate-nodegroup-cluster","title":"Step 6: Terminate NodeGroup &amp; Cluster","text":"<p>```bash</p>"},{"location":"eks/cluster/#get-available-clusters","title":"Get available clusters","text":"<p>$ eksctl get clusters</p>"},{"location":"eks/cluster/#get-avaiable-nodegroups-within-cluster","title":"Get avaiable nodegroups within cluster","text":"<p>$ eksctl get nodegroup --cluster="},{"location":"eks/cluster/#delete-nodegroup","title":"Delete Nodegroup","text":"<p>$ eksctl delete nodegroup --cluster= --name="},{"location":"eks/cluster/#once-all-nodegroup-are-deleted-delete-cluster","title":"Once all nodegroup are deleted, delete cluster","text":"<p>$ eksctl delete cluster"},{"location":"eks/ebs/ebs/","title":"Ebs","text":"Kubernetes Object YAML File Storage Class 01-storage-class.yml Persistent Volume Claim 02-persistent-volume-claim.yml Config Map 03-drop-db-config-map.yml Deployment, Environment Variables, Volumes, VolumeMounts 04-deployment.yml ClusterIP Service 05-service.yml"},{"location":"eks/ebs/ebs/#commands","title":"Commands","text":"<ol> <li>To provision the ff k8s objects above.     <pre><code>kubectl apply -f src/mysql\n</code></pre></li> <li>To verify the k8s objects creation.<ul> <li>Get Storage Classes    <pre><code>kubectl get sc\n</code></pre> <pre><code># Output\nNAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nebs-sc          ebs.csi.aws.com         Delete          WaitForFirstConsumer   false                  17m\n</code></pre></li> <li>Get Persistent Volume Claims   <pre><code>kubectl get pvc\n</code></pre> <pre><code># Output\nNAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nebs-mysql-pv-claim   Bound    pvc-2168b867-d7c2-4ee1-99aa-1109d90f1292   4Gi        RWO            ebs-sc         18m\n</code></pre></li> <li>Get Persistent Volume   <pre><code>kubectl get pv\n</code></pre> <pre><code># Output\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                        STORAGECLASS   REASON   AGE\npvc-2168b867-d7c2-4ee1-99aa-1109d90f1292   4Gi        RWO            Delete           Bound    default/ebs-mysql-pv-claim   ebs-sc                  17m\n</code></pre></li> <li>Get MySql Pod   <pre><code>kubectl get pods </code></pre> <pre><code># Output\nNAME                     READY   STATUS    RESTARTS   AGE\nmysql-58c65d6ff6-9g6m5   1/1     Running   0          19m\n</code></pre></li> </ul> </li> <li>To connect to database.     <pre><code>kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -ppassword\n</code></pre> <pre><code># Output\nIf you don't see a command prompt, try pressing enter.\n\nmysql&gt; </code></pre></li> <li>Show database databases.     <pre><code>show databases;\n</code></pre> <pre><code>mysql&gt; show databases;\n+---------------------+\n| Database            |\n+---------------------+\n| information_schema  |\n| #mysql50#lost+found |\n| mysql               |\n| performance_schema  |\n| usermgmt            |\n+---------------------+\n5 rows in set (0.00 sec)\nmysql&gt; </code></pre></li> </ol>"},{"location":"eks/ebs/prerequisite/","title":"Prerequisite","text":""},{"location":"eks/ebs/prerequisite/#amazon-ebs-csi-driver-set-up","title":"Amazon EBS CSI Driver Set-Up","text":"<p>Container Storage Interface (CSI) driver allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes.</p> <p></p>"},{"location":"eks/ebs/prerequisite/#step-1-create-ebs-csi-driver-iam-policy","title":"Step 1: Create EBS CSI DRIVER IAM Policy","text":"<ul> <li>Go to Services =&gt; IAM =&gt; Policies</li> <li>Create Policy and paste the permission below.     <pre><code>    {\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ec2:AttachVolume\",\n\"ec2:CreateSnapshot\",\n\"ec2:CreateTags\",\n\"ec2:CreateVolume\",\n\"ec2:DeleteSnapshot\",\n\"ec2:DeleteTags\",\n\"ec2:DeleteVolume\",\n\"ec2:DescribeInstances\",\n\"ec2:DescribeSnapshots\",\n\"ec2:DescribeTags\",\n\"ec2:DescribeVolumes\",\n\"ec2:DetachVolume\"\n],\n\"Resource\": \"*\"\n}\n]\n}\n</code></pre></li> <li>Name the policy as <code>k8s-amazon_ebs_csi_driver</code></li> <li>Add descrition: Policy for EC2 Instances to access Elastic Block Store.</li> <li>Create Policy.</li> </ul>"},{"location":"eks/ebs/prerequisite/#step-2-associate-it-with-node-iam-role","title":"Step 2:  Associate it with Node IAM Role","text":"<ul> <li>Get Worker node IAM Role ARN    <pre><code>kubectl -n kube-system describe configmap aws-auth\n</code></pre> <pre><code>Name:         aws-auth\n Namespace:    kube-system\n Labels:       &lt;none&gt;\n Annotations:  &lt;none&gt;\n\nData\n====\nmapRoles:\n ----\n - groups:\n - system:bootstrappers\n - system:nodes\n rolearn: arn:aws:iam::&lt;AWS-ACCOUNT_ID&gt;:role/eksctl-app-cluster-nodegroup-app-NodeInstanceRole-8RS8LUB7M4GX\n username: system:node:{{EC2PrivateDNSName}}\nBinaryData\n====\nEvents:  &lt;none&gt;\n</code></pre></li> <li>Search for it to Services =&gt; IAM =&gt; Roles</li> <li>Attach <code>k8s-amazon_ebs_csi_driver</code> Policy and Save.</li> </ul>"},{"location":"eks/ebs/prerequisite/#step-3-deploy-amazon-ebs-csi-driver","title":"Step 3: Deploy Amazon EBS CSI Driver","text":"<p>You can install it using AWS Manage Add-On directly to EKS cluster Managment console or manually create it and follow steps below</p> <ul> <li>Deploy EBS CSI Driver    <pre><code>kubectl apply -k \"github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=master\"\n</code></pre> <pre><code> serviceaccount/ebs-csi-controller-sa created\n serviceaccount/ebs-csi-node-sa created\n clusterrole.rbac.authorization.k8s.io/ebs-csi-node-role created\n clusterrole.rbac.authorization.k8s.io/ebs-external-attacher-role created\n clusterrole.rbac.authorization.k8s.io/ebs-external-provisioner-role created\n clusterrole.rbac.authorization.k8s.io/ebs-external-resizer-role created\n clusterrole.rbac.authorization.k8s.io/ebs-external-snapshotter-role created\n clusterrolebinding.rbac.authorization.k8s.io/ebs-csi-attacher-binding created\n clusterrolebinding.rbac.authorization.k8s.io/ebs-csi-node-getter-binding created\n clusterrolebinding.rbac.authorization.k8s.io/ebs-csi-provisioner-binding created\n clusterrolebinding.rbac.authorization.k8s.io/ebs-csi-resizer-binding created\n clusterrolebinding.rbac.authorization.k8s.io/ebs-csi-snapshotter-binding created\n deployment.apps/ebs-csi-controller created\n poddisruptionbudget.policy/ebs-csi-controller created\n daemonset.apps/ebs-csi-node created\n csidriver.storage.k8s.io/ebs.csi.aws.com created\n</code></pre></li> <li>Verify ebs-csi pods running    <pre><code>kubectl get pods -n kube-system\n</code></pre> <pre><code>NAME                                  READY   STATUS    RESTARTS   AGE\n aws-node-5lxx4                        1/1     Running   0          34m\n coredns-6c97f4f789-24j9d              1/1     Running   0          45m\n coredns-6c97f4f789-cpk4m              1/1     Running   0          45m\n ebs-csi-controller-5c67d6d86b-fb7fl   6/6     Running   0          2m34s\n ebs-csi-controller-5c67d6d86b-zbj2l   6/6     Running   0          2m34s\n ebs-csi-node-zql8v                    3/3     Running   0          2m34s\n kube-proxy-qczzj                      1/1     Running   0          34m\n</code></pre></li> </ul>"},{"location":"eks/ingress/","title":"Index","text":""},{"location":"eks/ingress/#ingress-controller-aws","title":"INGRESS CONTROLLER (AWS)","text":"<p>Anything within the red triangle is single object in kubernetes which is <code>ingress</code>.</p> <p></p> <p></p> <p>For us to manage the ingress in AWS, we need to configure the <code>AWS Load Balancer Controller/Ingress Controller</code>.</p> <ul> <li>How AWS Ingress Controller Works</li> <li>Configure AWS Ingress Controller</li> </ul>"},{"location":"eks/ingress/1-how-it-works/","title":"1 how it works","text":""},{"location":"eks/ingress/1-how-it-works/#how-kubernetes-ingress-works-with-aws-alb-ingress-controller","title":"How Kubernetes Ingress works with aws-alb-ingress-controller","text":"<p>The following diagram details the AWS components that the <code>aws-alb-ingress-controller</code> creates whenever an Ingress resource is defined by the user. The Ingress resource routes ingress traffic from the ALB to the Kubernetes cluster.</p>"},{"location":"eks/ingress/1-how-it-works/#ingress-creation-process","title":"Ingress Creation Process","text":"<p>Following the steps in the numbered blue circles in the above diagram:</p> <ol> <li> <p>The controller watches for Ingress events from the API server. When it finds Ingress resources that satisfy its requirements, it starts the creation of AWS resources. An ALB is created for the Ingress resource.</p> </li> <li> <p>An ALB is created for the Ingress resource.</p> </li> <li> <p>TargetGroups are created for each backend specified in the Ingress resource.</p> </li> <li> <p>Listeners are created for every port specified as Ingress resource annotation. If no port is specified, sensible defaults (80 or 443) are used.</p> </li> <li> <p>Rules are created for each path specified in your Ingress resource. This ensures that traffic to a specific path is routed to the correct <code>TargetGroup</code> created.</p> </li> </ol>"},{"location":"eks/ingress/1-how-it-works/#ingress-traffic","title":"Ingress Traffic","text":"<p>AWS ALB Ingress controller supports two traffic modes: instance mode and ip mode. Users can explicitly specify these traffic modes by declaring the alb.ingress.kubernetes.io/target-type annotation on the Ingress and the service definitions. </p> <ul> <li> <p><code>instance mode</code>: Ingress traffic starts from the ALB and reaches the NodePort opened for your service. Traffic is then routed to the pods within the cluster.</p> </li> <li> <p><code>ip mode</code>: Ingress traffic starts from the ALB and reaches the pods within the cluster directly. To use this mode, the networking plugin for the Kubernetes cluster must use a secondary IP address on ENI as pod IP, also known as the AWS CNI plugin for Kubernetes.</p> <p>Note: If you are using the ALB ingress with EKS on Fargate you want to use ip mode.</p> </li> </ul> <p></p>"},{"location":"eks/ingress/1-how-it-works/#references","title":"References:","text":"<ul> <li>Kubernetes Ingress with AWS ALB Ingress Controller</li> </ul>"},{"location":"eks/ingress/2-deploy-aws-alb-ingress-controller/","title":"2 deploy aws alb ingress controller","text":""},{"location":"eks/ingress/2-deploy-aws-alb-ingress-controller/#deploy-aws-load-balancer-controller","title":"Deploy AWS Load Balancer Controller","text":"<p>Let\u2019s deploy the AWS ALB Ingress controller into our EKS cluster using the steps below.</p>"},{"location":"eks/ingress/2-deploy-aws-alb-ingress-controller/#prerequisite","title":"Prerequisite","text":"<p>Note: Make sure that you've IAM OIDC provider associated with cluster. If not, kindly refer to cluster.md at Step 2: Creating an IAM OIDC provider for your cluster.**</p>"},{"location":"eks/ingress/2-deploy-aws-alb-ingress-controller/#create-service-account-with-iam-role-permissions-for-the-aws-load-balancer-controller","title":"Create Service Account with IAM Role &amp; Permissions for the AWS Load Balancer Controller","text":"<ul> <li> <p>Create an IAM policy named <code>AWSLoadBalancerControllerIAMPolicy</code> to allow the ALB Ingress controller to make AWS API calls on your behalf     <pre><code>aws iam create-policy \\\n--policy-name AWSLoadBalancerControllerIAMPolicy \\\n--policy-document https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json\n\n# If you want specific version: https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.3.1/docs/install/iam_policy.json\n</code></pre> <pre><code>// Output\n{\n\"Policy\": {\n\"PolicyName\": \"AWSLoadBalancerControllerIAMPolicy\", \"PermissionsBoundaryUsageCount\": 0, \"CreateDate\": \"2023-01-13T07:40:01Z\", \"AttachmentCount\": 0, \"IsAttachable\": true, \"PolicyId\": \"ANPAXE26SPP7QVXKG6HOK\", \"DefaultVersionId\": \"v1\", \"Path\": \"/\", \"Arn\": \"arn:aws:iam::491435228159:policy/AWSLoadBalancerControllerIAMPolicy\", \"UpdateDate\": \"2023-01-13T07:40:01Z\"\n}\n}\n</code></pre>     Record the <code>Policy.Arn</code> in the command output, you will need it in the next step </p> </li> <li> <p>Create a <code>Kubernetes service account</code> and an <code>IAM role</code> (for the pod running the AWS ALB Ingress controller) by substituting <code>$PolicyARN</code> with the recorded value from the previous step:     ```bash    eksctl create iamserviceaccount \\        --cluster=app-cluster \\        --namespace=kube-system \\        --name=aws-load-balancer-controller \\        --role-name=AWSLoadBalancerControllerIAMRole \\        --attach-policy-arn=arn:aws:iam::491435228159:policy/AWSLoadBalancerControllerIAMPolicy \\        --override-existing-serviceaccounts \\        --approve \\        --tags \"Project=Cap Build,Topic=EKS\"     <pre><code>```bash\n# Output\n2023-01-19 17:55:14 [\u2139]  1 iamserviceaccount (kube-system/aws-load-balancer-controller) was included (based on the include/exclude rules)\n2023-01-19 17:55:14 [!]  metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set\n2023-01-19 17:55:14 [\u2139]  1 task: { \n    2 sequential sub-tasks: { \n        create IAM role for serviceaccount \"kube-system/aws-load-balancer-controller\",\n        create serviceaccount \"kube-system/aws-load-balancer-controller\",\n    } }2023-01-19 17:55:14 [\u2139]  building iamserviceaccount stack \"eksctl-app-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\"\n2023-01-19 17:55:14 [\u2139]  deploying stack \"eksctl-app-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\"\n2023-01-19 17:55:14 [\u2139]  waiting for CloudFormation stack \"eksctl-app-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\"\n2023-01-19 17:55:45 [\u2139]  waiting for CloudFormation stack \"eksctl-app-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\"\n2023-01-19 17:56:42 [\u2139]  waiting for CloudFormation stack \"eksctl-app-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\"\n2023-01-19 17:56:43 [\u2139]  created serviceaccount \"kube-system/aws-load-balancer-controller\"\n</code></pre></p> <p>Verify k8s Service Account using kubectl. <pre><code>kubectl describe sa aws-load-balancer-controller -n kube-system </code></pre> <pre><code># Output\nName:                aws-load-balancer-controller\nNamespace:           kube-system\nLabels:              app.kubernetes.io/managed-by=eksctl\nAnnotations:         eks.amazonaws.com/role-arn: arn:aws:iam::491435228159:role/AWSLoadBalancerControllerIAMRole\nImage pull secrets:  &lt;none&gt;\nMountable secrets:   &lt;none&gt;\nTokens:              &lt;none&gt;\nEvents:              &lt;none&gt;\n</code></pre></p> </li> </ul>"},{"location":"eks/ingress/2-deploy-aws-alb-ingress-controller/#deploy-the-aws-load-balancer-controller","title":"Deploy the AWS Load Balancer Controller","text":"<ul> <li> <p>Install cert-manager so that you can inject the certificate configuration into the webhooks.</p> <p>cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates.</p> <p> <pre><code># Template\nkubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/$VERSION/cert-manager.yaml\n</code></pre>    For the <code>$VERSION</code>, you can refer to this link for the available version. For this tutorial, we'll use <code>v1.11.0</code>.    <pre><code>kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.11.0/cert-manager.yaml\n</code></pre> <pre><code># Output\nnamespace/cert-manager created\n customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created\n customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created\n customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created\n customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created\n customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created\n customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created\n serviceaccount/cert-manager-cainjector created\n serviceaccount/cert-manager created\n serviceaccount/cert-manager-webhook created\n configmap/cert-manager-webhook created\n clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created\n clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created\n clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\n clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created\n clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created\n clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created\n clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\n clusterrole.rbac.authorization.k8s.io/cert-manager-view created\n clusterrole.rbac.authorization.k8s.io/cert-manager-edit created\n clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\n clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\n clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\n clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created\n clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created\n clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\n clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created\n clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created\n clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created\n clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\n clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\n clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\n clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\n role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\n role.rbac.authorization.k8s.io/cert-manager:leaderelection created\n role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\n rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\n rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created\n rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\n service/cert-manager created\n service/cert-manager-webhook created\n deployment.apps/cert-manager-cainjector created\n deployment.apps/cert-manager created\n deployment.apps/cert-manager-webhook created\n mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\n validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\n</code></pre></p> </li> <li> <p>Download the manifest file for the AWS Load Balancer Controller from AWS GitHub, run the following command:     <pre><code># Template\ncurl -Lo ingress-controller.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/$VERSION/v2_4_1_full.yaml\n</code></pre>     For this tutorial, we'll use <code>v2.4.6</code> version. Change the <code>$VERSION</code> and <code>v2_4_1</code>_full.yaml based on version.       <code>bash     # Will download the content and save it to `ingress-controller.yaml`     curl -Lo ingress-controller.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.4.6/v2_4_6_full.yaml</code></p> </li> <li>Update the following <code>ingress-controller.yaml</code> information.<ul> <li>Edit the cluster-name for your cluster. Search for <code>your-cluster-name</code> and replace it with the name of cluster.   <pre><code>spec:\ncontainers:\n- args:\n- --cluster-name=app-cluster       # from your-cluster-name to app-cluster\n- --ingress-class=alb\nimage: public.ecr.aws/eks/aws-load-balancer-controller:v2.4.6\n</code></pre></li> <li>Update only the <code>ServiceAccount</code> section of the file only.   <pre><code>  apiVersion: v1\nkind: ServiceAccount\nmetadata:\nlabels:\napp.kubernetes.io/component: controller\napp.kubernetes.io/name: aws-load-balancer-controller\n# Add the annotations line,\n# so it won't override the created IAM role in the 1st step.    \nannotations:       eks.amazonaws.com/role-arn: arn:aws:iam::491435228159:role/AWSLoadBalancerControllerIAMRole             name: aws-load-balancer-controller\nnamespace: kube-system\n</code></pre></li> </ul> </li> <li> <p>Deploy AWS Load Balancer Controller    <pre><code>kubectl apply -f ingress-controller.yaml\n</code></pre> <pre><code># Output\ncustomresourcedefinition.apiextensions.k8s.io/ingressclassparams.elbv2.k8s.aws created\n customresourcedefinition.apiextensions.k8s.io/targetgroupbindings.elbv2.k8s.aws created\n Warning: resource serviceaccounts/aws-load-balancer-controller is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\n serviceaccount/aws-load-balancer-controller configured\n role.rbac.authorization.k8s.io/aws-load-balancer-controller-leader-election-role created\n clusterrole.rbac.authorization.k8s.io/aws-load-balancer-controller-role created\n rolebinding.rbac.authorization.k8s.io/aws-load-balancer-controller-leader-election-rolebinding created\n clusterrolebinding.rbac.authorization.k8s.io/aws-load-balancer-controller-rolebinding created\n service/aws-load-balancer-webhook-service created\n deployment.apps/aws-load-balancer-controller created\n certificate.cert-manager.io/aws-load-balancer-serving-cert created\n issuer.cert-manager.io/aws-load-balancer-selfsigned-issuer created\n mutatingwebhookconfiguration.admissionregistration.k8s.io/aws-load-balancer-webhook created\n validatingwebhookconfiguration.admissionregistration.k8s.io/aws-load-balancer-webhook created\n ingressclass.networking.k8s.io/alb created\n</code></pre></p> </li> <li> <p>Verify that the aws load controller is installed:     <pre><code>kubectl -n kube-system describe deployment aws-load-balancer-controller\n</code></pre> <pre><code># Output\nName:                   aws-load-balancer-controller\nNamespace:              kube-system\nCreationTimestamp:      Thu, 19 Jan 2023 18:34:28 +0800\nLabels:                 app.kubernetes.io/component=controller\n                        app.kubernetes.io/name=aws-load-balancer-controller\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               app.kubernetes.io/component=controller,app.kubernetes.io/name=aws-load-balancer-controller\nReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:           app.kubernetes.io/component=controller\n                    app.kubernetes.io/name=aws-load-balancer-controller\nService Account:  aws-load-balancer-controller\nContainers:\ncontroller:\n    Image:      public.ecr.aws/eks/aws-load-balancer-controller:v2.4.6\n    Port:       9443/TCP\n    Host Port:  0/TCP\n    Args:\n    --cluster-name=your-cluster-name\n    --ingress-class=alb\n    Limits:\n    cpu:     200m\n    memory:  500Mi\n    Requests:\n    cpu:        100m\n    memory:     200Mi\n    Liveness:     http-get http://:61779/healthz delay=30s timeout=10s period=10s #success=1 #failure=2\nEnvironment:  &lt;none&gt;\n    Mounts:\n    /tmp/k8s-webhook-server/serving-certs from cert (ro)\nVolumes:\ncert:\n    Type:               Secret (a volume populated by a Secret)\nSecretName:         aws-load-balancer-webhook-tls\n    Optional:           false\nPriority Class Name:  system-cluster-critical\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    NewReplicaSetAvailable\nOldReplicaSets:  &lt;none&gt;\nNewReplicaSet:   aws-load-balancer-controller-7858ff68b9 (1/1 replicas created)\nEvents:\nType    Reason             Age    From                   Message\n----    ------             ----   ----                   -------\nNormal  ScalingReplicaSet  3m32s  deployment-controller  Scaled up replica set aws-load-balancer-controller-7858ff68b9 to 1\n</code></pre></p> </li> <li> <p>Verify AWS Load Balancer Controller Webhook service created     <pre><code>kubectl -n kube-system get svc aws-load-balancer-webhook-service\n</code></pre> <pre><code># Output\nNAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\naws-load-balancer-webhook-service   ClusterIP   10.100.38.250   &lt;none&gt;        443/TCP   3m27s\n</code></pre></p> </li> <li> <p>Verify AWS Load Balancer Controller Logs     <pre><code># List Pods\nkubectl get pods -n kube-system | grep aws-load-balancer-controller\n\n# Output\naws-load-balancer-controller-7858ff68b9-njdw5   1/1     Running   0          5m27s\n</code></pre></p> <p><pre><code> kubectl -n kube-system logs -f aws-load-balancer-controller-7858ff68b9-njdw5\n</code></pre> <pre><code># Output\n{\"level\":\"info\",\"ts\":1673596695.9520242,\"msg\":\"version\",\"GitVersion\":\"v2.4.6\",\"GitCommit\":\"a92e689dfe464f5b24784f398947e0fef31dc470\",\"BuildDate\":\"2023-01-12T06:29:16+0000\"}\n{\"level\":\"info\",\"ts\":1673596695.9800384,\"logger\":\"controller-runtime.metrics\",\"msg\":\"metrics server is starting to listen\",\"addr\":\":8080\"}\n{\"level\":\"info\",\"ts\":1673596695.984325,\"logger\":\"setup\",\"msg\":\"adding health check for controller\"}\n{\"level\":\"info\",\"ts\":1673596695.9844737,\"logger\":\"controller-runtime.webhook\",\"msg\":\"registering webhook\",\"path\":\"/mutate-v1-pod\"}\n{\"level\":\"info\",\"ts\":1673596695.9845953,\"logger\":\"controller-runtime.webhook\",\"msg\":\"registering webhook\",\"path\":\"/mutate-elbv2-k8s-aws-v1beta1-targetgroupbinding\"}\n{\"level\":\"info\",\"ts\":1673596695.9847312,\"logger\":\"controller-runtime.webhook\",\"msg\":\"registering webhook\",\"path\":\"/validate-elbv2-k8s-aws-v1beta1-targetgroupbinding\"}\n{\"level\":\"info\",\"ts\":1673596695.9850075,\"logger\":\"controller-runtime.webhook\",\"msg\":\"registering webhook\",\"path\":\"/validate-networking-v1-ingress\"}\n{\"level\":\"info\",\"ts\":1673596695.9851177,\"logger\":\"setup\",\"msg\":\"starting podInfo repo\"}\nI0113 07:58:17.985462       1 leaderelection.go:243] attempting to acquire leader lease kube-system/aws-load-balancer-controller-leader...\n{\"level\":\"info\",\"ts\":1673596697.9858274,\"msg\":\"starting metrics server\",\"path\":\"/metrics\"}\n{\"level\":\"info\",\"ts\":1673596697.9858983,\"logger\":\"controller-runtime.webhook.webhooks\",\"msg\":\"starting webhook server\"}\n{\"level\":\"info\",\"ts\":1673596697.9862094,\"logger\":\"controller-runtime.certwatcher\",\"msg\":\"Updated current TLS certificate\"}\n{\"level\":\"info\",\"ts\":1673596697.9862907,\"logger\":\"controller-runtime.webhook\",\"msg\":\"serving webhook server\",\"host\":\"\",\"port\":9443}\n{\"level\":\"info\",\"ts\":1673596697.986575,\"logger\":\"controller-runtime.certwatcher\",\"msg\":\"Starting certificate watcher\"}\n</code></pre></p> </li> </ul>"},{"location":"eks/ingress/2-deploy-aws-alb-ingress-controller/#deploy-a-sample-application-to-test-the-aws-load-balancer-controller","title":"Deploy a sample application to test the AWS Load Balancer Controller","text":"<p>Deploy a sample application to verify that the AWS Load Balancer Controller creates a public Application Load Balancer because of the Ingress object.</p> <ul> <li>To deploy a game called 2048 as a sample application, run the following command:    <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/$VERSION/docs/examples/2048/2048_full.yaml\n</code></pre>    Note: Replace <code>$VERSION</code> with the version (from the Kubernetes SIGs GitHub site for example <code>v2.4.6</code>) of the AWS Load Balancer Controller that you want to deploy.    <pre><code> kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.6/docs/examples/2048/2048_full.yaml\n</code></pre> <pre><code># Output\nnamespace/game-2048 created\n deployment.apps/deployment-2048 created\n service/service-2048 created\n ingress.networking.k8s.io/ingress-2048 created\n</code></pre></li> <li>To verify that the Ingress resource was created, wait a few minutes, and then run the following command:      <pre><code>kubectl get ingress/ingress-2048 -n game-2048\n</code></pre> <pre><code># Output\nNAME           CLASS   HOSTS   ADDRESS                                                                      PORTS   AGE\ningress-2048   alb     *       k8s-game2048-ingress2-330cc1efad-65913070.ap-southeast-1.elb.amazonaws.com   80      93s\n</code></pre></li> <li>If your Ingress isn't created after several minutes, then run the following command to view the AWS Load Balancer Controller logs:   <pre><code>kubectl logs -n kube-system   deployment.apps/aws-load-balancer-controller\n</code></pre> Note: AWS Load Balancer Controller logs can show error messages to help you troubleshoot issues with your deployment.</li> </ul>"},{"location":"eks/ingress/2-deploy-aws-alb-ingress-controller/#uninstall-all-resources-created","title":"Uninstall all resources created.","text":"<ul> <li>Clean up the sample application     <pre><code>kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.6/docs/examples/2048/2048_full.yaml\n</code></pre></li> <li>Remove AWS Load Balancer Controller     <pre><code> kubectl delete -f ingress-controller.yaml\n</code></pre></li> <li>Remove Cert Manager     <pre><code> kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.11.0/cert-manager.yaml\n</code></pre></li> <li>Delete K8s IAM Service account     <pre><code>eksctl delete iamserviceaccount --cluster=app-cluster --name=aws-load-balancer-controller --namespace=kube-system  </code></pre></li> <li>Delete IAM Policy. Don't forget to change AWS ACCOUNT ID: <code>491435228159</code>.     <pre><code> aws iam delete-policy --policy-arn=arn:aws:iam::491435228159:policy/AWSLoadBalancerControllerIAMPolicy   </code></pre></li> </ul>"},{"location":"eks/ingress/2-deploy-aws-alb-ingress-controller/#references","title":"References:","text":"<ul> <li>https://aws.amazon.com/premiumsupport/knowledge-center/eks-alb-ingress-controller-setup/    </li> </ul>"},{"location":"eks/ingress/3-external-dns-controller/","title":"3 external dns controller","text":""},{"location":"eks/ingress/3-external-dns-controller/#external-dns-controller","title":"External DNS Controller","text":""},{"location":"eks/ingress/3-external-dns-controller/#create-iam-policy","title":"Create IAM Policy","text":"<p>This IAM policy will allow external-dns pod to add, remove DNS entries (Record Sets in a Hosted Zone) in AWS Route53 service.</p> <p><pre><code>    aws iam create-policy \\\n--policy-name AllowExternalDNSUpdates \\\n--policy-document file://manifest/external-dns-controller/policy.json\n</code></pre> <pre><code># Output\n{\n\"Policy\": {\n\"PolicyName\": \"AllowExternalDNSUpdates\", \"PermissionsBoundaryUsageCount\": 0, \"CreateDate\": \"2023-01-23T07:01:51Z\", \"AttachmentCount\": 0, \"IsAttachable\": true, \"PolicyId\": \"ANPAXE26SPP73NVLC4PWY\", \"DefaultVersionId\": \"v1\", \"Path\": \"/\", \"Arn\": \"arn:aws:iam::491435228159:policy/AllowExternalDNSUpdates\", \"UpdateDate\": \"2023-01-23T07:01:51Z\"\n}\n}\n</code></pre></p>"},{"location":"eks/ingress/3-external-dns-controller/#create-iam-role-k8s-service-account-associate-iam-policy","title":"Create IAM Role, k8s Service Account &amp; Associate IAM Policy","text":"<ul> <li>As part of this step, we are going to create a k8s Service Account named external-dns and also a AWS IAM role and associate them by annotating role ARN in Service Account.</li> <li>In addition, we are also going to associate the AWS IAM Policy AllowExternalDNSUpdates to the newly created AWS IAM Role.</li> </ul> <p><pre><code>eksctl create iamserviceaccount \\\n--name external-dns \\\n--role-name=IngressExternalDNSRole \\\n--namespace default \\\n--cluster app-cluster \\\n--attach-policy-arn arn:aws:iam::491435228159:policy/AllowExternalDNSUpdates \\\n--approve \\\n--override-existing-serviceaccounts\n</code></pre> <pre><code># Output\n2023-01-23 15:04:44 [\u2139]  1 existing iamserviceaccount(s) (kube-system/aws-load-balancer-controller) will be excluded\n2023-01-23 15:04:44 [\u2139]  1 iamserviceaccount (default/external-dns) was included (based on the include/exclude rules)\n2023-01-23 15:04:44 [!]  metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set\n2023-01-23 15:04:44 [\u2139]  1 task: { 2 sequential sub-tasks: { create IAM role for serviceaccount \"default/external-dns\",\n        create serviceaccount \"default/external-dns\",\n    } }2023-01-23 15:04:44 [\u2139]  building iamserviceaccount stack \"eksctl-app-cluster-addon-iamserviceaccount-default-external-dns\"\n2023-01-23 15:04:44 [\u2139]  deploying stack \"eksctl-app-cluster-addon-iamserviceaccount-default-external-dns\"\n2023-01-23 15:04:44 [\u2139]  waiting for CloudFormation stack \"eksctl-app-cluster-addon-iamserviceaccount-default-external-dns\"\n2023-01-23 15:05:14 [\u2139]  waiting for CloudFormation stack \"eksctl-app-cluster-addon-iamserviceaccount-default-external-dns\"\n2023-01-23 15:06:05 [\u2139]  waiting for CloudFormation stack \"eksctl-app-cluster-addon-iamserviceaccount-default-external-dns\"\n2023-01-23 15:06:05 [\u2139]  created serviceaccount \"default/external-dns\"\n</code></pre></p> <p><pre><code>eksctl get iamserviceaccount --cluster app-cluster\n</code></pre> <pre><code># Output\nNAMESPACE       NAME                            ROLE ARN\ndefault         external-dns                    arn:aws:iam::491435228159:role/IngressExternalDNSRole\nkube-system     aws-load-balancer-controller    arn:aws:iam::491435228159:role/AWSLoadBalancerControllerIAMRole\n</code></pre></p>"},{"location":"eks/ingress/3-external-dns-controller/#deploy-external-dns-controller","title":"Deploy External DNS Controller","text":"<ul> <li>Update External DNS Kubernetes manifest    File Location: <code>manifest/external-dns/controller.yaml</code></li> </ul> <p>Change the ff:     - The created Service Account IAM role on the previous step since we don't want to override it once we run the manifest file.       <pre><code># Line 9\neks.amazonaws.com/role-arn: arn:aws:iam::491435228159:role/AWSLoadBalancerControllerIAMRole\n</code></pre>     - We used eksctl to create IAM role and attached the AllowExternalDNSUpdates policy &amp; we didnt use KIAM or Kube2IAM so we don't need these two lines, so commented       <pre><code># Commented line 55 &amp; 56\n#annotations:  \n  #iam.amazonaws.com/role: arn:aws:iam::ACCOUNT-ID:role/IAM-SERVICE-ROLE-NAME  \n</code></pre> <pre><code> # Commented line 65 &amp; 67\n # - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n  # - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n</code></pre>     - Get latest Docker Image version <pre><code>    spec:\n    serviceAccountName: external-dns\n    containers:\n    - name: external-dns\n        image: k8s.gcr.io/external-dns/external-dns:v0.13.2\n</code></pre>  - Deploy external-dns manifest file    <pre><code> kubectl -f manifest/external-dns/controller.yaml\n</code></pre> <pre><code> serviceaccount/external-dns configured\n clusterrole.rbac.authorization.k8s.io/external-dns created\n clusterrolebinding.rbac.authorization.k8s.io/external-dns-viewer created\n</code></pre></p> <ul> <li>To verify the external DNS creation, you can view the external-dns controller.    <pre><code>kubectl logs -f $(kubectl get po | egrep -o 'external-dns[A-Za-z0-9-]+')\n</code></pre></li> </ul>"},{"location":"eks/ingress/3-external-dns-controller/#clean-up","title":"Clean Up","text":"<ul> <li>External DNS Controller    <pre><code> kubectl delete -f manifest/external-dns/controller.yaml\n</code></pre></li> <li>External DNS Service Account     <pre><code> eksctl delete iamserviceaccount --cluster=app-cluster --name=external-dns --namespace=default </code></pre></li> <li>Delete IAM Policy. Don't forget to change AWS ACCOUNT ID: <code>491435228159</code>.     <pre><code> aws iam delete-policy --policy-arn=arn:aws:iam::491435228159:policy/AllowExternalDNSUpdates   </code></pre></li> </ul>"},{"location":"eks/ingress/3-external-dns-controller/#references","title":"References","text":"<ul> <li>https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/alb-ingress.md</li> <li>https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md</li> </ul>"},{"location":"eks/ingress/4-external-dns/","title":"4 external dns","text":""},{"location":"eks/ingress/4-external-dns/#external-dns","title":"External DNS","text":""},{"location":"eks/ingress/4-external-dns/#for-ingress","title":"For Ingress","text":"<ul> <li>Add External DNS Annotation     <pre><code># External DNS - For creating a Record Set in Route53\nexternal-dns.alpha.kubernetes.io/hostname: k8s.solutions-chapters.click\n</code></pre></li> <li>Deploy your changes     <pre><code>kubectl -f &lt;ingress&gt;.yaml\n</code></pre></li> </ul>"},{"location":"eks/ingress/4-external-dns/#for-service","title":"For Service","text":"<ul> <li>Add External DNS Annotation     <pre><code># External DNS - For creating a Record Set in Route53\nexternal-dns.alpha.kubernetes.io/hostname: k8s.solutions-chapters.click\n</code></pre></li> <li>Make sure the <code>Service Type</code> is <code>LoadBalancer</code>.     <pre><code># Example\napiVersion: v1\nkind: Service\nmetadata:\nname: app1-nginx-loadbalancer-service\nlabels:\napp: app1-nginx\nannotations:\nexternal-dns.alpha.kubernetes.io/hostname: k8s.solutions-chapters.click\nspec:\ntype: LoadBalancer # &lt;=\nselector:\napp: app1-nginx\nports:\n- port: 80\ntargetPort: 80  </code></pre></li> </ul>"},{"location":"eks/ingress/4-external-dns/#verify-dns-log","title":"Verify DNS Log","text":"<pre><code>kubectl logs -f $(kubectl get po | egrep -o 'external-dns[A-Za-z0-9-]+')\n</code></pre>"},{"location":"eks/ingress/manifest/4-ingress/","title":"Index","text":""},{"location":"eks/ingress/manifest/4-ingress/#ingress","title":"Ingress","text":""},{"location":"eks/ingress/manifest/4-ingress/#high-level-diagram","title":"High-Level Diagram","text":""},{"location":"eks/ingress/manifest/4-ingress/#routes","title":"Routes","text":"<ul> <li><code>/app1/*</code> - will route to nginx-app1</li> <li><code>/app2/*</code> - will route to nginx-app2</li> <li><code>/*</code> - will route to nginx-app3</li> </ul> <p>Respective annotation alb.ingress.<code>kubernetes.io/healthcheck-path:</code> will be moved to respective application <code>NodePort Service</code>.</p>"},{"location":"eks/ingress/manifest/4-ingress/#ssl-high-level-network-diagram","title":"SSL High-Level Network Diagram","text":"<p>To enable this, please follow 3-external-dns-controller &amp; 4-external-dns.md.</p>"},{"location":"eks/ingress/manifest/4-ingress/#references","title":"References","text":"<ul> <li>Ingress Annotations</li> </ul>"},{"location":"images/ReadMe/","title":"ReadMe","text":"<p>ReadMe</p>"},{"location":"k8s/namespace/","title":"Namespace","text":""},{"location":"k8s/namespace/#namespace","title":"Namespace","text":"<p>Provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc).</p>"},{"location":"k8s/namespace/#setting-the-namespace-preference","title":"Setting the namespace preference","text":"<pre><code>$ kubectl config set-context --current --namespace=&lt;insert-namespace-name-here&gt;\n# Validate it\n$ kubectl config view --minify | grep namespace:\n</code></pre>"},{"location":"k8s/namespace/#namespaces-and-dns","title":"Namespaces and DNS","text":"<p>When you create a Service, it creates a corresponding DNS entry. This entry is of the form <code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local</code>, which means that if a container only uses , it will resolve to the service which is local to a namespace. This is useful for using the same configuration across multiple namespaces such as Development, Staging and Production. If you want to reach across namespaces, you need to use the fully qualified domain name (FQDN)."},{"location":"k8s/object/","title":"Object","text":""},{"location":"k8s/object/#k8s-object","title":"K8s Object","text":"<p>The term object is a reference to a thing that exists inside K8s cluster.</p> <p>Object serve different purposes such as:    - running containers    - monitoring containers    - setting up networking    - etc.</p> <p>Object Types  - StatefulSet  - ReplicaController  - Pod - runs one or more very closely related containers.(Use Deployment Instead of Pod)  - Deployment - Maintains a set of identical pods, ensuring that they have the correct config and the right number exists.</p> <ul> <li>Service - Sets up networking in a K8s cluster.</li> <li>ClusterIP - Exposes a set of pods to other objects in the cluster.</li> <li>NodePort - Exposes a container to the outside world (Only good for dev purposes!!!)</li> <li>LoadBalancer</li> <li>Ingress </li> </ul>"},{"location":"k8s/object/#pods-vs-deployments","title":"Pods vs Deployments","text":"<ul> <li>Pods</li> <li>Runs a single set of containers</li> <li>Good for one-off dev purposes</li> <li>Rarely used directly in production</li> <li>Deployment</li> <li>Runs a set of identical pods (1 or more)</li> <li>Monitors the state of each pod, updating as necessary.</li> <li>Good for dev &amp; production</li> </ul>"},{"location":"k8s/object/#api-version","title":"API VERSION","text":"<p>Scopes or limit types of objects that we can specify that we want to create with any configuration file.</p> <p>Each API version defines a different set of 'object' we can use.</p> <ul> <li>apiVersion: v1</li> <li>componentStatus</li> <li>configMap</li> <li>Endpoints</li> <li>Event</li> <li>Namespace</li> <li> <p>Pod</p> </li> <li> <p>apiVersion: apps/v1</p> </li> <li>ControllerRevision</li> <li>StatefulSet</li> <li>Deployment</li> </ul>"},{"location":"k8s/pods/","title":"Pods","text":""},{"location":"k8s/pods/#pods","title":"PODS","text":""},{"location":"k8s/pods/#create-a-pod-imperative-approach","title":"Create a Pod (Imperative Approach)","text":"<pre><code>kubectl run pod-nginx --image butch/sdc-nginx:1.0.0\n</code></pre>"},{"location":"k8s/pods/#get-list-of-pods","title":"Get List of Pods","text":"<p><pre><code>kubectl get pods\n</code></pre> <pre><code>Output: NAME        READY   STATUS    RESTARTS   AGE\npod-nginx   1/1     Running   0          2m17s\n</code></pre></p>"},{"location":"k8s/pods/#to-get-more-information","title":"To Get More Information","text":"<p><pre><code>kubectl describe pod pod-nginx\n</code></pre> <pre><code># Output\nName:         pod-nginx\nNamespace:    default\nPriority:     0\nNode:         minikube/192.168.49.2\nStart Time:   Fri, 06 Jan 2023 12:29:39 +0800\nLabels:       run=pod-nginx\nAnnotations:  &lt;none&gt;\nStatus:       Running\nIP:           172.17.0.17\nIPs:\n  IP:  172.17.0.17\nContainers:\n  pod-nginx:\n    Container ID:   docker://4cf374fe28c7cfea72e9c9e099de1fcafdf9929e9376d10f90e0a1464d5f654d\n    Image:          butch/sdc-nginx:1.0.0\n    Image ID:       docker-pullable://butch/sdc-nginx@sha256:6550554427248228c13cf8f8a95dac922c142ddb947db938add3d4637a6be9cd\n    Port:           &lt;none&gt;\n    Host Port:      &lt;none&gt;\n    State:          Running\n      Started:      Fri, 06 Jan 2023 12:29:52 +0800\n    Ready:          True\n    Restart Count:  0\nEnvironment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wljl2 (ro)\nConditions:\n  Type              Status\n  Initialized       True Ready             True ContainersReady   True PodScheduled      True Volumes:\n  kube-api-access-wljl2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\nTokenExpirationSeconds:  3607\nConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       &lt;nil&gt;\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              &lt;none&gt;\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age    From               Message\n  ----    ------     ----   ----               -------\n  Normal  Scheduled  4m27s  default-scheduler  Successfully assigned default/pod-nginx to minikube\n  Normal  Pulling    4m27s  kubelet            Pulling image \"butch/sdc-nginx:1.0.0\"\nNormal  Pulled     4m16s  kubelet            Successfully pulled image \"butch/sdc-nginx:1.0.0\" in 11.486351163s\n  Normal  Created    4m15s  kubelet            Created container pod-nginx\n  Normal  Started    4m15s  kubelet            Started container pod-nginx\n</code></pre></p>"},{"location":"k8s/pods/#access-application","title":"Access Application","text":"<ul> <li>To Access application, we need to create K8s Service.   <pre><code>kubectl expose pod pod-nginx --type=NodePort --port=80 --name=svc-nginx </code></pre></li> <li>Get Service Info   <pre><code>kubectl get service\n</code></pre> <pre><code># Output\nNAME        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nsvc-nginx   NodePort   10.110.22.228   &lt;none&gt;        80:30821/TCP   88s\n</code></pre></li> <li>Get Public IP of Worker Node    <pre><code>kubectl get nodes -o wide\n</code></pre> <pre><code># Output\nNAME       STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nminikube   Ready    control-plane   23d   v1.25.3   192.168.49.2   &lt;none&gt;        Ubuntu 20.04.5 LTS   5.15.0-57-generic   docker://20.10.20\n</code></pre></li> <li>Access Using Node Public IP + NodePort   <pre><code>curl -vvv http://192.168.49.2:30821\n</code></pre> <pre><code># Output\n*   Trying 192.168.49.2:30821...\n* Connected to 192.168.49.2 (192.168.49.2) port 30821 (#0)\n&gt; GET / HTTP/1.1\n&gt; Host: 192.168.49.2:30821\n&gt; User-Agent: curl/7.81.0\n&gt; Accept: */*\n&gt; * Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 OK\n&lt; Server: nginx/1.18.0\n&lt; Date: Fri, 06 Jan 2023 04:44:49 GMT\n&lt; Content-Type: text/html\n&lt; Content-Length: 612\n&lt; Last-Modified: Sun, 07 Jun 2020 07:48:56 GMT\n&lt; Connection: keep-alive\n&lt; ETag: \"5edc9be8-264\"\n&lt; Accept-Ranges: bytes\n&lt; &lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\n    body {\nwidth: 35em;\nmargin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif;\n}\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n* Connection #0 to host 192.168.49.2 left intact\n</code></pre></li> </ul>"},{"location":"k8s/pods/#check-pod-logs","title":"Check Pod Logs","text":"<p><pre><code># Normal checking the pods.\nkubectl logs pod-nginx\n</code></pre> <pre><code># Unfortunately, we're not sending logs to console, instead check the actual log file.\nkubectl exec pod-nginx -- cat /var/log/nginx/access.log\n</code></pre> <pre><code># Output\n172.17.0.1 - - [06/Jan/2023:04:42:50 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n172.17.0.1 - - [06/Jan/2023:04:42:50 +0000] \"GET /favicon.ico HTTP/1.1\" 404 555 \"http://192.168.49.2:30821/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n172.17.0.1 - - [06/Jan/2023:04:44:49 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.81.0\"\n</code></pre></p>"},{"location":"k8s/pods/#get-yaml-output-of-pod-service-declarative-approach","title":"Get YAML Output of Pod &amp; Service (Declarative Approach)","text":"<ul> <li>Pod: pod-nginx   <pre><code># Get pod definition YAML output\nkubectl get pod pod-nginx -o yaml &gt; pod-nginx.yaml\n\n# Check pod-nginx.yaml\ncat pod-nginx.yaml\n</code></pre> <pre><code># Output\napiVersion: v1\n  kind: Pod\n  metadata:\n    creationTimestamp: \"2023-01-06T04:29:39Z\"\nlabels:\n      run: pod-nginx\n    name: pod-nginx\n    namespace: default\n    resourceVersion: \"222530\"\nuid: 913212ea-78ac-43e4-becb-434f86e2324c\n  spec:\n    containers:\n    - image: butch/sdc-nginx:1.0.0\n      imagePullPolicy: IfNotPresent\n      name: pod-nginx\n      resources: {}\nterminationMessagePath: /dev/termination-log\n      terminationMessagePolicy: File\n      volumeMounts:\n      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n        name: kube-api-access-wljl2\n        readOnly: true\ndnsPolicy: ClusterFirst\n    enableServiceLinks: true\nnodeName: minikube\n    preemptionPolicy: PreemptLowerPriority\n    priority: 0\nrestartPolicy: Always\n    schedulerName: default-scheduler\n    securityContext: {}\nserviceAccount: default\n    serviceAccountName: default\n    terminationGracePeriodSeconds: 30\ntolerations:\n    - effect: NoExecute\n      key: node.kubernetes.io/not-ready\n      operator: Exists\n      tolerationSeconds: 300\n- effect: NoExecute\n      key: node.kubernetes.io/unreachable\n      operator: Exists\n      tolerationSeconds: 300\nvolumes:\n    - name: kube-api-access-wljl2\n      projected:\n        defaultMode: 420\nsources:\n        - serviceAccountToken:\n            expirationSeconds: 3607\npath: token\n        - configMap:\n            items:\n            - key: ca.crt\n              path: ca.crt\n            name: kube-root-ca.crt\n        - downwardAPI:\n            items:\n            - fieldRef:\n                apiVersion: v1\n                fieldPath: metadata.namespace\n              path: namespace\n  status:\n    conditions:\n    - lastProbeTime: null\n      lastTransitionTime: \"2023-01-06T04:29:39Z\"\nstatus: \"True\"\ntype: Initialized\n    - lastProbeTime: null\n      lastTransitionTime: \"2023-01-06T04:29:53Z\"\nstatus: \"True\"\ntype: Ready\n    - lastProbeTime: null\n      lastTransitionTime: \"2023-01-06T04:29:53Z\"\nstatus: \"True\"\ntype: ContainersReady\n    - lastProbeTime: null\n      lastTransitionTime: \"2023-01-06T04:29:39Z\"\nstatus: \"True\"\ntype: PodScheduled\n    containerStatuses:\n    - containerID: docker://4cf374fe28c7cfea72e9c9e099de1fcafdf9929e9376d10f90e0a1464d5f654d\n      image: butch/sdc-nginx:1.0.0\n      imageID: docker-pullable://butch/sdc-nginx@sha256:6550554427248228c13cf8f8a95dac922c142ddb947db938add3d4637a6be9cd\n      lastState: {}\nname: pod-nginx\n      ready: true\nrestartCount: 0\nstarted: true\nstate:\n        running:\n          startedAt: \"2023-01-06T04:29:52Z\"\nhostIP: 192.168.49.2\n    phase: Running\n    podIP: 172.17.0.17\n    podIPs:\n    - ip: 172.17.0.17\n    qosClass: BestEffort\n    startTime: \"2023-01-06T04:29:39Z\"\n</code></pre></li> <li>Service: svc-nginx     <pre><code># Get Service Definition\nkubectl get service svc-nginx -o yaml &gt; svc-nginx.yaml\n\n# Check Service: svc-nginx\ncat  svc-nginx.yaml\n</code></pre> <pre><code># Output\napiVersion: v1\n  kind: Service\n  metadata:\n    creationTimestamp: \"2023-01-06T04:37:33Z\"\nlabels:\n      run: pod-nginx\n    name: svc-nginx\n    namespace: default\n    resourceVersion: \"222919\"\nuid: 007ebfe3-6617-4369-ae33-cd362ce270d4\n  spec:\n    clusterIP: 10.110.22.228\n    clusterIPs:\n    - 10.110.22.228\n    externalTrafficPolicy: Cluster\n    internalTrafficPolicy: Cluster\n    ipFamilies:\n    - IPv4\n    ipFamilyPolicy: SingleStack\n    ports:\n    - nodePort: 30821\nport: 80\nprotocol: TCP\n      targetPort: 80\nselector:\n      run: pod-nginx\n    sessionAffinity: None\n    type: NodePort\n  status:\n    loadBalancer: {}\n</code></pre></li> </ul> <p>### Clean Up  <pre><code>kubectl delete pod pod-nginx\n# Output: pod \"pod-nginx\" deleted\nkubectl delete service svc-nginx\n# Output: service \"svc-nginx\" deleted\n</code></pre></p>"},{"location":"k8s/deployment/1-creation/","title":"1 creation","text":""},{"location":"k8s/deployment/1-creation/#deployment","title":"Deployment","text":""},{"location":"k8s/deployment/1-creation/#create-deployment","title":"Create Deployment","text":"<pre><code>kubectl create deployment nginx-deployment --image butch/sdc-nginx:1.0.0\n# Output: deployment.apps/nginx-deployment created\n</code></pre>"},{"location":"k8s/deployment/1-creation/#verify-created-deployment","title":"Verify Created Deployment","text":"<ul> <li>Get Deployment: nginx-deployment     <pre><code>kubectl get deployment nginx-deployment\n</code></pre> <pre><code># Output:\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\n    nginx-deployment   1/1     1            1           2m3s\n</code></pre></li> <li>Describe Deployment     <pre><code>kubectl describe deployment nginx-deployment\n</code></pre> <pre><code># Output:\nName:                   nginx-deployment\nNamespace:              default\nCreationTimestamp:      Fri, 06 Jan 2023 14:11:57 +0800\nLabels:                 app=nginx-deployment\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               app=nginx-deployment\nReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx-deployment\nContainers:\nsdc-nginx:\n    Image:        butch/sdc-nginx:1.0.0\n    Port:         &lt;none&gt;\n    Host Port:    &lt;none&gt;\n    Environment:  &lt;none&gt;\n    Mounts:       &lt;none&gt;\nVolumes:        &lt;none&gt;\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    NewReplicaSetAvailable\nOldReplicaSets:  &lt;none&gt;\nNewReplicaSet:   nginx-deployment-7d6c9564c (1/1 replicas created)\nEvents:\nType    Reason             Age   From                   Message\n----    ------             ----  ----                   -------\nNormal  ScalingReplicaSet  4m1s  deployment-controller  Scaled up replica set nginx-deployment-7d6c9564c to 1\n</code></pre></li> <li>Verify created ReplicaSet    <pre><code>kubectl get replicaset\n</code></pre> <pre><code># Output\nNAME                         DESIRED   CURRENT   READY   AGE\nnginx-deployment-7d6c9564c   1         1         1       7m36s\n</code></pre></li> </ul>"},{"location":"k8s/deployment/1-creation/#scaling-updown-deployment","title":"Scaling Up/Down Deployment","text":"<p><pre><code>kubectl scale --replicas=5 deployment/nginx-deployment\n# Output: deployment.apps/nginx-deployment scaled\n</code></pre> <pre><code># Verify the deployments\nkubectl get deployment\n</code></pre> <pre><code># Output\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   5/5     5            5           13m\n</code></pre></p>"},{"location":"k8s/deployment/1-creation/#expose-deployment-as-service","title":"Expose Deployment as Service","text":"<p><pre><code>kubectl expose deployment nginx-deployment --type=NodePort --port=80 --target-port=80 --name=nginx-svc\n# Output: service/nginx-svc exposed\n</code></pre> <pre><code># Get Service Info\nkubectl get service nginx-svc\n</code></pre> <pre><code># Output\nNAME        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nnginx-svc   NodePort   10.110.230.53   &lt;none&gt;        80:30075/TCP   84s\n</code></pre></p>"},{"location":"k8s/deployment/1-creation/#access-application","title":"Access Application","text":"<p><pre><code>kubectl get nodes -o wide\n</code></pre> <pre><code># Output\nNAME       STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nminikube   Ready    control-plane   23d   v1.25.3   192.168.49.2   &lt;none&gt;        Ubuntu 20.04.5 LTS   5.15.0-57-generic   docker://20.10.20\n</code></pre></p> <p><pre><code># Get nginx-svc assigned NodePort\nkubectl get service nginx-svc\n</code></pre> <pre><code># Output\nNAME        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nnginx-svc   NodePort   10.110.230.53   &lt;none&gt;        80:30075/TCP   5m9s\n</code></pre> <pre><code> curl http://192.168.49.2:30075/ -vvv\n</code></pre> <pre><code># Output\n*   Trying 192.168.49.2:30075...\n    * Connected to 192.168.49.2 (192.168.49.2) port 30075 (#0)\n&gt; GET / HTTP/1.1\n    &gt; Host: 192.168.49.2:30075\n    &gt; User-Agent: curl/7.81.0\n    &gt; Accept: */*\n    &gt; * Mark bundle as not supporting multiuse\n    &lt; HTTP/1.1 200 OK\n    &lt; Server: nginx/1.18.0\n    &lt; Date: Fri, 06 Jan 2023 06:34:19 GMT\n    &lt; Content-Type: text/html\n    &lt; Content-Length: 612\n&lt; Last-Modified: Sun, 07 Jun 2020 07:48:56 GMT\n    &lt; Connection: keep-alive\n    &lt; ETag: \"5edc9be8-264\"\n&lt; Accept-Ranges: bytes\n    &lt; &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n    &lt;title&gt;Welcome to nginx!&lt;/title&gt;\n    &lt;style&gt;\n        body {\nwidth: 35em;\nmargin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif;\n}\n&lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n    &lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n    &lt;p&gt;If you see this page, the nginx web server is successfully installed and\n    working. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n    &lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\n    Commercial support is available at\n    &lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    * Connection #0 to host 192.168.49.2 left intact\n</code></pre></p>"},{"location":"k8s/deployment/1-creation/#clean-up","title":"Clean Up","text":"<p><pre><code>kubectl delete service nginx-svc\n# Output: service \"nginx-svc\" deleted\n</code></pre> <pre><code>kubectl delete deployment nginx-deployment\n# Output: deployment.apps \"nginx-deployment\" deleted\n</code></pre></p>"},{"location":"k8s/deployment/2-update/","title":"2 update","text":""},{"location":"k8s/deployment/2-update/#deployment","title":"Deployment","text":""},{"location":"k8s/deployment/2-update/#edit-deployment","title":"Edit Deployment","text":"<p><pre><code>kubectl edit deployment/nginx-deployment\n</code></pre> <pre><code># It will show the deployment metadata in yaml format.\n# This is not the full metadata, and just look for container image and set it from butch/sdc-nginx:1.0.0 to nginx:alpine\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nannotations:\ndeployment.kubernetes.io/revision: \"1\"\ncreationTimestamp: \"2023-01-06T06:41:20Z\"\ngeneration: 2\nlabels:\napp: nginx-deployment\nname: nginx-deployment\nnamespace: default\nresourceVersion: \"229345\"\nuid: 7b249702-c162-47ef-b55d-a9c2d8265f7d\nspec:\nprogressDeadlineSeconds: 600\nreplicas: 5\nrevisionHistoryLimit: 10\nselector:\nmatchLabels:\napp: nginx-deployment\nstrategy:\nrollingUpdate:\nmaxSurge: 25%\nmaxUnavailable: 25%\ntype: RollingUpdate\ntemplate:\nmetadata:\ncreationTimestamp: null\nlabels:\napp: nginx-deployment\nspec:\ncontainers:\n- image: nginx:alpine\nimagePullPolicy: IfNotPresent\nname: sdc-nginx\nresources: {}\nterminationMessagePath: /dev/termination-log\nterminationMessagePolicy: File\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nschedulerName: default-scheduler\nsecurityContext: {}\nterminationGracePeriodSeconds: 30\n</code></pre></p>"},{"location":"k8s/deployment/2-update/#verify-rollout-status","title":"Verify Rollout Status","text":"<p>Observation: Rollout happens in a rolling update model, so no downtime.</p> <pre><code>kubectl rollout status deployment/nginx-deployment\n# Output: deployment \"nginx-deployment\" successfully rolled out\n</code></pre>"},{"location":"k8s/deployment/2-update/#verify-rollout-history","title":"Verify Rollout History","text":"<p><pre><code>kubectl rollout history deployment/nginx-deployment\n</code></pre> <pre><code># Output\nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         &lt;none&gt;\n</code></pre></p>"},{"location":"k8s/deployment/2-update/#access-application","title":"Access Application","text":"<p><pre><code>kubectl get nodes -o wide\n</code></pre> <pre><code># Output\nNAME       STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nminikube   Ready    control-plane   23d   v1.25.3   192.168.49.2   &lt;none&gt;        Ubuntu 20.04.5 LTS   5.15.0-57-generic   docker://20.10.20\n</code></pre></p> <p><pre><code># Get nginx-svc assigned NodePort\nkubectl get service nginx-svc\n</code></pre> <pre><code># Output\nNAME        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nnginx-svc   NodePort   10.110.230.53   &lt;none&gt;        80:30075/TCP   5m9s\n</code></pre> <pre><code> curl http://192.168.49.2:30075/ -vvv\n</code></pre> <pre><code># Output\n*   Trying 192.168.49.2:30075...\n    * Connected to 192.168.49.2 (192.168.49.2) port 30075 (#0)\n&gt; GET / HTTP/1.1\n    &gt; Host: 192.168.49.2:30075\n    &gt; User-Agent: curl/7.81.0\n    &gt; Accept: */*\n    &gt; * Mark bundle as not supporting multiuse\n    &lt; HTTP/1.1 200 OK\n    &lt; Server: nginx/1.18.0\n    &lt; Date: Fri, 06 Jan 2023 06:34:19 GMT\n    &lt; Content-Type: text/html\n    &lt; Content-Length: 612\n&lt; Last-Modified: Sun, 07 Jun 2020 07:48:56 GMT\n    &lt; Connection: keep-alive\n    &lt; ETag: \"5edc9be8-264\"\n&lt; Accept-Ranges: bytes\n    &lt; &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n    &lt;title&gt;Welcome to nginx!&lt;/title&gt;\n    &lt;style&gt;\n        body {\nwidth: 35em;\nmargin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif;\n}\n&lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n    &lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n    &lt;p&gt;If you see this page, the nginx web server is successfully installed and\n    working. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n    &lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\n    Commercial support is available at\n    &lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    * Connection #0 to host 192.168.49.2 left intact\n</code></pre></p>"},{"location":"k8s/deployment/2-update/#clean-up","title":"Clean Up","text":"<p><pre><code>kubectl delete service nginx-svc\n# Output: service \"nginx-svc\" deleted\n</code></pre> <pre><code>kubectl delete deployment nginx-deployment\n# Output: deployment.apps \"nginx-deployment\" deleted\n</code></pre></p>"},{"location":"k8s/deployment/3-rollback/","title":"3 rollback","text":""},{"location":"k8s/deployment/3-rollback/#deployment","title":"Deployment","text":""},{"location":"k8s/deployment/3-rollback/#rollback-deployment","title":"Rollback Deployment","text":"<ul> <li>Check rollback history     <pre><code>kubectl rollout history deployment/nginx-deployment\n</code></pre> <pre><code># Output\nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         &lt;none&gt;\n</code></pre></li> <li>Verify Changes<ul> <li>Revision 1     <pre><code>kubectl rollout history deployment/nginx-deployment --revision=1\n</code></pre> <pre><code># Ouput\nPod Template:\nLabels: app=nginx-deployment\npod-template-hash=7d6c9564c\nContainers:\nsdc-nginx:\nImage:  butch/sdc-nginx:1.0.0\nPort:   &lt;none&gt;\nHost Port:  &lt;none&gt;\nEnvironment:    &lt;none&gt;\nMounts: &lt;none&gt;\nVolumes:    &lt;none&gt;\n</code></pre></li> <li>Revision 2     <pre><code>kubectl rollout history deployment/nginx-deployment --revision=2\n</code></pre> <pre><code># Output\nPod Template:\nLabels: app=nginx-deployment\npod-template-hash=66d5c8bfc5\nContainers:\nsdc-nginx:\nImage:  nginx:alpine\nPort:   &lt;none&gt;\nHost Port:  &lt;none&gt;\nEnvironment:    &lt;none&gt;\nMounts: &lt;none&gt;\nVolumes:    &lt;none&gt;\n</code></pre></li> </ul> </li> <li>Rollback to previous version     <pre><code>kubectl rollout undo deployment/nginx-deployment --to-revision=1\n# Output: deployment.apps/nginx-deployment rolled back\n</code></pre></li> <li>Verify Deployment     <pre><code>kubectl describe deployment nginx-deployment\n</code></pre> <pre><code># Output\nName:                   nginx-deployment\nNamespace:              default\nCreationTimestamp:      Fri, 06 Jan 2023 14:41:20 +0800\nLabels:                 app=nginx-deployment\nAnnotations:            deployment.kubernetes.io/revision: 3\nSelector:               app=nginx-deployment\nReplicas:               5 desired | 5 updated | 5 total | 5 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx-deployment\nContainers:\nsdc-nginx:\n    Image:        butch/sdc-nginx:1.0.0\n    Port:         &lt;none&gt;\n    Host Port:    &lt;none&gt;\n    Environment:  &lt;none&gt;\n    Mounts:       &lt;none&gt;\nVolumes:        &lt;none&gt;\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    NewReplicaSetAvailable\nOldReplicaSets:  &lt;none&gt;\nNewReplicaSet:   nginx-deployment-7d6c9564c (5/5 replicas created)\nEvents:\nType    Reason             Age   From                   Message\n----    ------             ----  ----                   -------\nNormal  ScalingReplicaSet  29m   deployment-controller  Scaled up replica set nginx-deployment-7d6c9564c to 1\nNormal  ScalingReplicaSet  29m   deployment-controller  Scaled up replica set nginx-deployment-7d6c9564c to 5 from 1\nNormal  ScalingReplicaSet  19m   deployment-controller  Scaled up replica set nginx-deployment-66d5c8bfc5 to 2\nNormal  ScalingReplicaSet  19m   deployment-controller  Scaled down replica set nginx-deployment-7d6c9564c to 4 from 5\nNormal  ScalingReplicaSet  19m   deployment-controller  Scaled up replica set nginx-deployment-66d5c8bfc5 to 3 from 2\nNormal  ScalingReplicaSet  19m   deployment-controller  Scaled down replica set nginx-deployment-7d6c9564c to 3 from 4\nNormal  ScalingReplicaSet  19m   deployment-controller  Scaled up replica set nginx-deployment-66d5c8bfc5 to 4 from 3\nNormal  ScalingReplicaSet  19m   deployment-controller  Scaled down replica set nginx-deployment-7d6c9564c to 2 from 3\nNormal  ScalingReplicaSet  19m   deployment-controller  Scaled up replica set nginx-deployment-66d5c8bfc5 to 5 from 4\nNormal  ScalingReplicaSet  19m   deployment-controller  Scaled down replica set nginx-deployment-7d6c9564c to 1 from 2\nNormal  ScalingReplicaSet  18m   deployment-controller  Scaled down replica set nginx-deployment-7d6c9564c to 0 from 1\nNormal  ScalingReplicaSet  81s   deployment-controller  Scaled up replica set nginx-deployment-7d6c9564c to 2 from 0\nNormal  ScalingReplicaSet  81s   deployment-controller  Scaled down replica set nginx-deployment-66d5c8bfc5 to 4 from 5\nNormal  ScalingReplicaSet  81s   deployment-controller  Scaled up replica set nginx-deployment-7d6c9564c to 3 from 2\nNormal  ScalingReplicaSet  79s   deployment-controller  Scaled down replica set nginx-deployment-66d5c8bfc5 to 3 from 4\nNormal  ScalingReplicaSet  79s   deployment-controller  Scaled up replica set nginx-deployment-7d6c9564c to 4 from 3\nNormal  ScalingReplicaSet  79s   deployment-controller  Scaled down replica set nginx-deployment-66d5c8bfc5 to 2 from 3\nNormal  ScalingReplicaSet  79s   deployment-controller  Scaled up replica set nginx-deployment-7d6c9564c to 5 from 4\nNormal  ScalingReplicaSet  78s   deployment-controller  Scaled down replica set nginx-deployment-66d5c8bfc5 to 1 from 2\nNormal  ScalingReplicaSet  76s   deployment-controller  Scaled down replica set nginx-deployment-66d5c8bfc5 to 0 from 1\n</code></pre></li> </ul>"},{"location":"k8s/deployment/Readme/","title":"Readme","text":""},{"location":"k8s/deployment/Readme/#deployment","title":"Deployment","text":"<p>Another layer on top of ReplicaSet that provides out-of-the-box functionality specifically in terms of \"Deployments\".</p> <ul> <li>ReplicaSet Deployment Management<ul> <li>Rollout</li> <li>Updating</li> <li>Scaling</li> <li>Rolling Back</li> <li>Pause/Resume</li> <li>Status</li> <li>Clean up Policy</li> <li>Canary Deployments</li> </ul> </li> </ul>"}]}